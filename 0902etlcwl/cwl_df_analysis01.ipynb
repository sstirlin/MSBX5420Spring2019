{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CWL data analysis using DataFrames\n",
    "\n",
    "In this homework you will ETL the Call of Duty World League Championship data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F  # will be used a LOT\n",
    "from pyspark import Row  # Row will be used in some of the assertions\n",
    "\n",
    "ss = SparkSession.builder.\\\n",
    "     master('spark://spark-master:7077').\\\n",
    "     appName('cwlanalysis').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's validate that you successfully uploaded all of the CWL data to HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "client = InsecureClient('http://namenode:50070', user='vagrant')\n",
    "cwldirs = client.list('/Users/vagrant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "50d36bc67f8beb19cd3c57db53ad7920",
     "grade": true,
     "grade_id": "cell-5159cc0bfe660f88",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 'structured-2018-08-19-champs' in cwldirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's cleanup any junk parquet files that you might have already in HDFS\n",
    "client.delete('/Users/vagrant/matches_df.parquet', recursive=True)\n",
    "client.delete('/Users/vagrant/teammatches_df.parquet', recursive=True)\n",
    "client.delete('/Users/vagrant/modes_df.parquet', recursive=True)\n",
    "client.delete('/Users/vagrant/playermatches_df.parquet', recursive=True)\n",
    "client.delete('/Users/vagrant/matchevents_df.parquet', recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the \"champs\" dataset (each json file = 1 match played = 1 row in the DataFrame):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ss.read.json('hdfs://namenode/Users/vagrant/structured-2018-08-19-champs/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.count() == 296"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort your DataFrame by the `id` column (in ascending order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1c8375bef0ee05b2f3beee2cce426345",
     "grade": false,
     "grade_id": "cell-a26cdd11c9eef9a7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few ways to check out what the DataFrame looks like.  The first is probably to just list out the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, this doesn't reveal much about *nested* structure.  It is probably better to list out the schema.  There are a few ways to do this.  The first is to use Python lingo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second is to list out the schema in Scala lingo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh my, THAT^^ is ugly.  Fortunately, there is a \"pretty print\" version of this that we will be much more useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `matches_df`\n",
    "\n",
    "The columns `events`, `players`, `teams`, and `hp_hill_names` are arrays (lists). We will want to \"explode\" each of them into their own tables.  Later we can analyze using table joins (just like in SQL).\n",
    "\n",
    "Let's first create a table called `matches_df` that omits these arrays.  Use the DataFrames `.drop()` function to drop these 4 columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a06adfe5ea2e1cd2d8166fdff102e15f",
     "grade": false,
     "grade_id": "cell-9355377a778d462c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5a087606a88470002cd3550b8e14c70a",
     "grade": true,
     "grade_id": "cell-92bdff73ba2bcb9a",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert matches_df.take(2) == \\\n",
    "[Row(duration_ms=522000, end_time_s=1534359399, hp_hill_rotations=9, id='0066bbc8-4e5f-5641-9224-c743c1b003dc', map='London Docks', mode='Hardpoint', platform='ps4', rounds=1, series_id='champs-pool-F-1', start_time_s=1534358877, title='ww2'),\n",
    " Row(duration_ms=569000, end_time_s=1534364215, hp_hill_rotations=None, id='006a2f3e-b942-564e-9515-3a2fbff1a817', map='USS Texas', mode='Search & Destroy', platform='ps4', rounds=8, series_id='champs-pool-H-1', start_time_s=1534363646, title='ww2')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how to visualize a couple of rows using `.take(5)`/`.head(5)`, `.show(5)`, and, if the DataFrame is small enough, `.collect()`.\n",
    "\n",
    "A better viewing experience is to use `.limit(5)` (which builds a DataFrame of only 5 elements in Spark) and then `.toPandas()` to convert it to a Pandas DataFrame for viewing on the driver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are running DANGEROUSLY low on memory right now.  Let's write this DataFrame out to HDFS and delete it.  We'll read it back in later when we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df.write.parquet('hdfs://namenode/Users/vagrant/matches_df.parquet')\n",
    "del matches_df\n",
    "ss.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `modes_df`\n",
    "\n",
    "Let's see what game modes were being played in CWL in 2018.  Recall that Call of Duty is really a collection of many games, each inspired by games that kids play on the playground (e.g. King of the Hill, Capture the Flag, etc).\n",
    "\n",
    "Create a DataFrame named `modes_df` where each row is a distinct game mode.  Make sure they are sorted alphabetically.  Use Spark to do the sorting (hint:  how would you do this in SQL?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4b2d840f33c4d08af9fbf2e84485d012",
     "grade": false,
     "grade_id": "cell-97b929d13eb7dedf",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f0303d395756322e20f9dfa5a980156b",
     "grade": true,
     "grade_id": "cell-1fb6c25b8193bdb2",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert modes_df.collect() == \\\n",
    "[Row(mode='Capture The Flag'),\n",
    " Row(mode='Hardpoint'),\n",
    " Row(mode='Search & Destroy')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are running DANGEROUSLY low on memory right now.  Let's write this DataFrame out to HDFS and delete it.  We'll read it back in later when we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes_df.write.parquet('hdfs://namenode/Users/vagrant/modes_df.parquet')\n",
    "del modes_df\n",
    "ss.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `teammatches_df`\n",
    "\n",
    "In the original DataFrame (`df` above) the `teams` column was really an array containing the statistics for the two teams that played against each other in that match.\n",
    "\n",
    "We want to use the `explode` function to expand elements in the array to individual rows in a new table.  Your new DataFrame should be named `teammatches_df` and contain three columns:\n",
    "\n",
    "- `id` (so that you can join back to other tables)\n",
    "- `mode` (it is useful to store this redundantly in this table so that we can cut down on expensive joins later)\n",
    "- `team` (contains the struct for a single team).\n",
    "\n",
    "`team` will still be nested (i.e. it will contain fields like `team.name` and `team.is_victor`).\n",
    "\n",
    "Make sure that your new DataFrame is sorted in ascending order by match `id`, and then by team name.\n",
    "\n",
    "Hints:  we did `import pyspark.sql.functions as F` above for a reason.  Also, you can `.alias` a column.  This was shown in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a5ccfcba3800b785278f6f4b626f9ffa",
     "grade": false,
     "grade_id": "cell-9cca51138fb61e6f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f50b39554e57226d06c2716fc57b6eae",
     "grade": true,
     "grade_id": "cell-beeeca09f5276d50",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert teammatches_df.take(2) == \\\n",
    "[Row(id='0066bbc8-4e5f-5641-9224-c743c1b003dc', mode='Hardpoint', team=Row(is_victor=False, name='TEAM PRISMATIC', round_scores=[14, 39, 2, 7, 2, 0, 0, 33, 0], score=97, side='home')),\n",
    " Row(id='0066bbc8-4e5f-5641-9224-c743c1b003dc', mode='Hardpoint', team=Row(is_victor=True, name='UNILAD', round_scores=[20, 5, 51, 26, 29, 37, 44, 9, 29], score=250, side='away'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the schema so that we can refer to it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teammatches_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are running DANGEROUSLY low on memory right now.  Let's write this DataFrame out to HDFS and delete it.  We'll read it back in later when we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teammatches_df.write.parquet('hdfs://namenode/Users/vagrant/teammatches_df.parquet')\n",
    "del teammatches_df\n",
    "ss.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `playermatches_df`\n",
    "\n",
    "We want to similarly explode the `players` column in the original DataFrame (`df`) into a new DataFrame that we'll call `playermatches_df`.\n",
    "\n",
    "Each row will contain the statistics for a single player in a single match.\n",
    "\n",
    "As we did for `teammatches_df`, let's have 3 columns:\n",
    "\n",
    "- `id`\n",
    "- `mode`\n",
    "- `player`\n",
    "\n",
    "where `player` is the exploded column.\n",
    "\n",
    "Make sure that new DataFrame is sorted by `id` (first) and then by player's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "89c9aa34c7777800ccb48a1cba3c436a",
     "grade": false,
     "grade_id": "cell-49a42ffd9d754555",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dcff5c6b2a7d420bd616ae4b12a87af4",
     "grade": true,
     "grade_id": "cell-c70f72e23d13e7ba",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "grab2rows = playermatches_df.take(2)\n",
    "assert grab2rows[0].id == '0066bbc8-4e5f-5641-9224-c743c1b003dc'\n",
    "assert grab2rows[1].id == '0066bbc8-4e5f-5641-9224-c743c1b003dc'\n",
    "assert grab2rows[0]['player']['name'] == 'ALEX'\n",
    "assert grab2rows[1]['player']['name'] == 'MALLS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playermatches_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are running DANGEROUSLY low on memory right now.  Let's write this DataFrame out to HDFS and delete it.  We'll read it back in later when we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playermatches_df.write.parquet('hdfs://namenode/Users/vagrant/playermatches_df.parquet')\n",
    "del playermatches_df\n",
    "ss.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `matchevents_df`\n",
    "\n",
    "We want to similarly explode the `events` column in the original DataFrame (`df`) into a new DataFrame that we'll call `matchevents_df`.\n",
    "\n",
    "Each row will contain the statistics for a single event in a single match.\n",
    "\n",
    "As we did for `teammatches_df`, let's have 3 columns:\n",
    "\n",
    "- `id`\n",
    "- `mode`\n",
    "- `event`\n",
    "\n",
    "where `event` is the exploded column.\n",
    "\n",
    "Make sure that new DataFrame is sorted by `id` (first) and then by the time from the start of the match (hint:  `time_ms` measures this).  Note that this does not specify a unique ordering (since several events in a match might occur at the exact same time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5f25ed785af97f5822045db6e3de1740",
     "grade": false,
     "grade_id": "cell-511f380709bf4047",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a31639a8fe3af672d50e3ae5cc21551e",
     "grade": true,
     "grade_id": "cell-28d1f7bb41c2eeff",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "grab2rows = matchevents_df.take(2)\n",
    "assert grab2rows[0].id == '0066bbc8-4e5f-5641-9224-c743c1b003dc'\n",
    "assert grab2rows[1].id == '0066bbc8-4e5f-5641-9224-c743c1b003dc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchevents_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are running DANGEROUSLY low on memory right now.  Let's write this DataFrame out to HDFS and delete it.  We'll read it back in later when we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchevents_df.write.parquet('hdfs://namenode/Users/vagrant/matchevents_df.parquet')\n",
    "del matchevents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean up as much as possible\n",
    "del df\n",
    "ss.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
